{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio google-generativeai PyPDF2 langchain langchain-community langchain-chroma sentence-transformers -q\n",
        "\n"
      ],
      "metadata": {
        "id": "P58tvOtG0zL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  📘 DocuLingo\n",
        "# İngilizce PDF dosyalarını Türkçe veya İngilizce özetleyen RAG tabanlı chatbot\n",
        "# ============================================================\n",
        "\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import tempfile, os\n",
        "\n",
        "# ==============================\n",
        "# 🔑 API Anahtarını Al\n",
        "# ==============================\n",
        "GOOGLE_API_KEY = input(\"🔑 Gemini API Key'inizi girin: \").strip()\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# ==============================\n",
        "# 🧠 Yardımcı Fonksiyonlar\n",
        "# ==============================\n",
        "\n",
        "def summarize_pdf(pdf_file, lang):\n",
        "    \"\"\"PDF dosya yolunu alır, özet çıkarır.\"\"\"\n",
        "    pdf_path = pdf_file  # artık doğrudan path geliyor, read() yok\n",
        "\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        content = page.extract_text()\n",
        "        if content:\n",
        "            text += content + \"\\n\"\n",
        "\n",
        "\n",
        "    # PDF içeriğini çıkar\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        content = page.extract_text()\n",
        "        if content:\n",
        "            text += content + \"\\n\"\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-2.0-flash\", generation_config={\"temperature\": 0.3, \"max_output_tokens\": 1500})\n",
        "\n",
        "    if lang == \"Türkçe\":\n",
        "        prompt = f\"\"\"\n",
        "        Aşağıdaki akademik makaleyi Türkçe olarak özetle.\n",
        "        Çalışmanın amacı, yöntemleri, bulguları ve sonuçlarını açıkla.\n",
        "        En az 6-7 cümlelik detaylı bir özet yaz.\n",
        "        Makale:\n",
        "        {text[:7000]}\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"\n",
        "        Summarize the following academic paper in English.\n",
        "        Explain the goal, methods, findings, and conclusions in detail (6–7 sentences).\n",
        "        Paper:\n",
        "        {text[:7000]}\n",
        "        \"\"\"\n",
        "\n",
        "    summary = model.generate_content(prompt)\n",
        "    summary = getattr(summary, \"text\", str(summary))\n",
        "\n",
        "    # Chroma veritabanı oluştur (RAG için)\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
        "    chunks = splitter.split_text(text)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vectorstore = Chroma.from_texts(chunks, embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "    return summary, vectorstore\n",
        "\n",
        "\n",
        "def chat_with_paper(question, vectorstore, lang):\n",
        "    \"\"\"Kullanıcının sorusuna makale tabanlı yanıt ver\"\"\"\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        google_api_key=GOOGLE_API_KEY,\n",
        "        temperature=0.2,\n",
        "        system_prompt=\"You are an academic assistant that only answers based on the provided paper content.\"\n",
        "    )\n",
        "\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        return_source_documents=False\n",
        "    )\n",
        "\n",
        "    result = qa.invoke({\"question\": question, \"chat_history\": []})\n",
        "    return result[\"answer\"]\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# 🧩 Gradio Arayüzü\n",
        "# ==============================\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## 📘 DocuLingo\")\n",
        "    gr.Markdown(\"**Yapay zekâ destekli özet ve soru-cevap aracı.** Türkçe veya İngilizce olarak makale özetleyebilir ve sorularınızı yanıtlayabilir.\")\n",
        "\n",
        "    lang_choice = gr.Radio([\"Türkçe\", \"İngilizce\"], label=\"🌍 Özet Dili Seçin\")\n",
        "    pdf_input = gr.File(label=\"📂 PDF Dosyası Yükle\", file_types=[\".pdf\"], type=\"filepath\")\n",
        "    summarize_btn = gr.Button(\"🧠 Özeti Oluştur\")\n",
        "\n",
        "    summary_output = gr.Textbox(label=\"📄 Özet\", lines=10)\n",
        "    chat_input = gr.Textbox(label=\"❓ Sorunuzu Yazın\")\n",
        "    chat_btn = gr.Button(\"💬 Sor\")\n",
        "    chat_output = gr.Textbox(label=\"🤖 Cevap\", lines=8)\n",
        "\n",
        "    state_vectorstore = gr.State()\n",
        "\n",
        "    def process_and_summarize(pdf_file, lang):\n",
        "        summary, vectorstore = summarize_pdf(pdf_file, lang)\n",
        "        return summary, vectorstore\n",
        "\n",
        "    summarize_btn.click(\n",
        "        process_and_summarize,\n",
        "        inputs=[pdf_input, lang_choice],\n",
        "        outputs=[summary_output, state_vectorstore]\n",
        "    )\n",
        "\n",
        "    chat_btn.click(\n",
        "        chat_with_paper,\n",
        "        inputs=[chat_input, state_vectorstore, lang_choice],\n",
        "        outputs=chat_output\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BoomeRLJ04Rr",
        "outputId": "ba6d5c76-12ec-4af5-983b-27fb99b9995e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Gemini API Key'inizi girin: AIzaSyAplGeHrM5F8DPButbbEL6iQeyQ5ZrXXgQ\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://fe8a738ea995a006e9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fe8a738ea995a006e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 389, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x78c678e57890 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://fe8a738ea995a006e9.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DocuLingo (arayüzsüz)\n",
        "# ============================================================\n",
        "\n",
        "!pip install google-generativeai PyPDF2 langchain langchain-community langchain-google-genai chromadb python-dotenv -q\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "import google.generativeai as genai\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from google.colab import files, output\n",
        "from IPython.display import display, HTML\n",
        "import time, os\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 🔑 Gemini API Key\n",
        "# ============================================================\n",
        "GOOGLE_API_KEY = input(\"🔑 Gemini API Key'inizi girin: \").strip()\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# ============================================================\n",
        "# 📂 PDF Yükleme\n",
        "# ============================================================\n",
        "print(\"\\n📁 Lütfen özetlenecek PDF dosyasını yükleyin:\")\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "# PDF metnini oku\n",
        "reader = PdfReader(pdf_path)\n",
        "text = \"\"\n",
        "for page in reader.pages:\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        text += content + \"\\n\"\n",
        "\n",
        "# ============================================================\n",
        "# 🌍 Dil Seçimi (Kullanıcıdan Buton ile)\n",
        "# ============================================================\n",
        "print(\"📘 Özet hangi dilde olsun?\")\n",
        "print(\"1️⃣ Türkçe\\n2️⃣ İngilizce\")\n",
        "\n",
        "choice = input(\"Seçiminizi yapın (1 veya 2): \").strip()\n",
        "if choice == \"1\":\n",
        "    chosen_lang = \"tr\"\n",
        "elif choice == \"2\":\n",
        "    chosen_lang = \"en\"\n",
        "else:\n",
        "    print(\"❌ Geçersiz seçim, varsayılan Türkçe seçildi.\")\n",
        "    chosen_lang = \"tr\"\n",
        "\n",
        "print(f\"🔤 Seçilen dil: {chosen_lang}\")\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ✨ Özet Oluşturma (Gemini)\n",
        "# ============================================================\n",
        "model_name = \"gemini-2.0-flash\"\n",
        "model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0.3, \"max_output_tokens\": 1500})\n",
        "\n",
        "if chosen_lang == \"tr\":\n",
        "   prompt = f\"\"\"\n",
        "    Aşağıdaki akademik makalenin ayrıntılı bir özetini oluştur.\n",
        "    Çalışmanın amacı, kullanılan yöntemler, veriler, bulgular ve sonuçları açıklayan\n",
        "    en az 6-8 cümlelik akademik bir paragraf yaz.\n",
        "    Ayrıca kullanılan modelleri veya algoritmaları da belirt.\n",
        "    Türkçe yaz.\n",
        "\n",
        "    Makale metni:\n",
        "    {text[:7000]}\n",
        "    \"\"\"\n",
        "else:\n",
        "   prompt = f\"\"\"\n",
        "    Provide a detailed summary of the following academic paper.\n",
        "    Include the research goal, methodology, data used, findings, and conclusions.\n",
        "    Write an academic-style paragraph with at least 6–8 sentences.\n",
        "    Also mention any models or algorithms used.\n",
        "    Write in English.\n",
        "\n",
        "    Paper text:\n",
        "    {text[:7000]}\n",
        "    \"\"\"\n",
        "\n",
        "print(\"\\n🤖 Özeti oluşturuyor, lütfen bekleyin...\")\n",
        "summary = model.generate_content(prompt)\n",
        "try:\n",
        "    summary = summary.text\n",
        "except:\n",
        "    summary = str(summary)\n",
        "\n",
        "print(\"\\n📄 ÖZET:\")\n",
        "print(\"────────────────────────────────────────────\")\n",
        "print(summary)\n",
        "print(\"────────────────────────────────────────────\")\n",
        "\n",
        "# ============================================================\n",
        "# 🧩 ChromaDB + Soru Cevap\n",
        "# ============================================================\n",
        "print(\"\\n🔍 Makale içeriği dizine ekleniyor...\")\n",
        "CHROMA_DIR = \"chroma_db\"\n",
        "os.makedirs(CHROMA_DIR, exist_ok=True)\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "embedding_model = \"models/embedding-001\"\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# HuggingFace ücretsiz embedding modeli\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = Chroma.from_texts(chunks, embeddings, persist_directory=CHROMA_DIR)\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=model_name,\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.2,\n",
        "    system_prompt=\"You are an AI academic assistant. Only answer based on the provided paper content.\"\n",
        ")\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
        "\n",
        "print(\"\\n💬 Artık makaleyle ilgili sorular sorabilirsiniz!\")\n",
        "print(\"Çıkmak için 'q' yazın.\\n\")\n",
        "\n",
        "while True:\n",
        "    question = input(\"❓ Soru: \")\n",
        "    if question.lower() == \"q\":\n",
        "        print(\"🔚 Sohbet sonlandırıldı.\")\n",
        "        break\n",
        "    result = qa.invoke({\"question\": question, \"chat_history\": []})\n",
        "    print(\"\\n🧠 Cevap:\\n\", result[\"answer\"])\n",
        "    print(\"────────────────────────────────────────────\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "IJ639tFEhy1V",
        "outputId": "7f1ef939-54c4-4407-ce0d-ecd4f9ded29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 Gemini API Key'inizi girin: AIzaSyAplGeHrM5F8DPButbbEL6iQeyQ5ZrXXgQ\n",
            "\n",
            "📁 Lütfen özetlenecek PDF dosyasını yükleyin:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a4e6be20-8425-4d67-9d10-0bc0900804ed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a4e6be20-8425-4d67-9d10-0bc0900804ed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Chapter 5 Elasticity and Its Application.pdf to Chapter 5 Elasticity and Its Application.pdf\n",
            "📘 Özet hangi dilde olsun?\n",
            "1️⃣ Türkçe\n",
            "2️⃣ İngilizce\n",
            "Seçiminizi yapın (1 veya 2): 2\n",
            "🔤 Seçilen dil: en\n",
            "\n",
            "🤖 Özeti oluşturuyor, lütfen bekleyin...\n",
            "\n",
            "📄 ÖZET:\n",
            "────────────────────────────────────────────\n",
            "This academic paper, presented as a PowerPoint by Andreea Chiritescu, explores the concept of elasticity, specifically focusing on the price elasticity of demand. The primary research goal is to define and explain the concept of price elasticity of demand, its determinants, and its implications for total revenue. The methodology employed is primarily descriptive, outlining the key concepts and providing examples to illustrate different types of demand elasticity. The data used is hypothetical, presented in the form of demand curves and numerical examples to demonstrate the calculation of price elasticity using the midpoint method. The findings highlight that the price elasticity of demand is influenced by factors such as the availability of substitutes, whether a good is a necessity or a luxury, the definition of the market, and the time horizon. The paper concludes that understanding price elasticity is crucial for businesses as it directly impacts total revenue; for instance, an inelastic demand implies that a price increase will raise total revenue, while an elastic demand suggests the opposite. The paper uses the midpoint method formula to calculate the price elasticity of demand between two points on the demand curve.\n",
            "\n",
            "────────────────────────────────────────────\n",
            "\n",
            "🔍 Makale içeriği dizine ekleniyor...\n",
            "\n",
            "💬 Artık makaleyle ilgili sorular sorabilirsiniz!\n",
            "Çıkmak için 'q' yazın.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1084204386.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"❓ Soru: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"q\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🔚 Sohbet sonlandırıldı.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}