{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio google-generativeai PyPDF2 langchain langchain-community langchain-chroma sentence-transformers -q\n",
        "\n"
      ],
      "metadata": {
        "id": "P58tvOtG0zL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  ğŸ“˜ DocuLingo\n",
        "# Ä°ngilizce PDF dosyalarÄ±nÄ± TÃ¼rkÃ§e veya Ä°ngilizce Ã¶zetleyen RAG tabanlÄ± chatbot\n",
        "# ============================================================\n",
        "\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import tempfile, os\n",
        "\n",
        "# ==============================\n",
        "# ğŸ”‘ API AnahtarÄ±nÄ± Al\n",
        "# ==============================\n",
        "GOOGLE_API_KEY = input(\"ğŸ”‘ Gemini API Key'inizi girin: \").strip()\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# ==============================\n",
        "# ğŸ§  YardÄ±mcÄ± Fonksiyonlar\n",
        "# ==============================\n",
        "\n",
        "def summarize_pdf(pdf_file, lang):\n",
        "    \"\"\"PDF dosya yolunu alÄ±r, Ã¶zet Ã§Ä±karÄ±r.\"\"\"\n",
        "    pdf_path = pdf_file  # artÄ±k doÄŸrudan path geliyor, read() yok\n",
        "\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        content = page.extract_text()\n",
        "        if content:\n",
        "            text += content + \"\\n\"\n",
        "\n",
        "\n",
        "    # PDF iÃ§eriÄŸini Ã§Ä±kar\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        content = page.extract_text()\n",
        "        if content:\n",
        "            text += content + \"\\n\"\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-2.0-flash\", generation_config={\"temperature\": 0.3, \"max_output_tokens\": 1500})\n",
        "\n",
        "    if lang == \"TÃ¼rkÃ§e\":\n",
        "        prompt = f\"\"\"\n",
        "        AÅŸaÄŸÄ±daki akademik makaleyi TÃ¼rkÃ§e olarak Ã¶zetle.\n",
        "        Ã‡alÄ±ÅŸmanÄ±n amacÄ±, yÃ¶ntemleri, bulgularÄ± ve sonuÃ§larÄ±nÄ± aÃ§Ä±kla.\n",
        "        En az 6-7 cÃ¼mlelik detaylÄ± bir Ã¶zet yaz.\n",
        "        Makale:\n",
        "        {text[:7000]}\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"\n",
        "        Summarize the following academic paper in English.\n",
        "        Explain the goal, methods, findings, and conclusions in detail (6â€“7 sentences).\n",
        "        Paper:\n",
        "        {text[:7000]}\n",
        "        \"\"\"\n",
        "\n",
        "    summary = model.generate_content(prompt)\n",
        "    summary = getattr(summary, \"text\", str(summary))\n",
        "\n",
        "    # Chroma veritabanÄ± oluÅŸtur (RAG iÃ§in)\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
        "    chunks = splitter.split_text(text)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vectorstore = Chroma.from_texts(chunks, embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "    return summary, vectorstore\n",
        "\n",
        "\n",
        "def chat_with_paper(question, vectorstore, lang):\n",
        "    \"\"\"KullanÄ±cÄ±nÄ±n sorusuna makale tabanlÄ± yanÄ±t ver\"\"\"\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        google_api_key=GOOGLE_API_KEY,\n",
        "        temperature=0.2,\n",
        "        system_prompt=\"You are an academic assistant that only answers based on the provided paper content.\"\n",
        "    )\n",
        "\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        return_source_documents=False\n",
        "    )\n",
        "\n",
        "    result = qa.invoke({\"question\": question, \"chat_history\": []})\n",
        "    return result[\"answer\"]\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# ğŸ§© Gradio ArayÃ¼zÃ¼\n",
        "# ==============================\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"## ğŸ“˜ DocuLingo\")\n",
        "    gr.Markdown(\"**Yapay zekÃ¢ destekli Ã¶zet ve soru-cevap aracÄ±.** TÃ¼rkÃ§e veya Ä°ngilizce olarak makale Ã¶zetleyebilir ve sorularÄ±nÄ±zÄ± yanÄ±tlayabilir.\")\n",
        "\n",
        "    lang_choice = gr.Radio([\"TÃ¼rkÃ§e\", \"Ä°ngilizce\"], label=\"ğŸŒ Ã–zet Dili SeÃ§in\")\n",
        "    pdf_input = gr.File(label=\"ğŸ“‚ PDF DosyasÄ± YÃ¼kle\", file_types=[\".pdf\"], type=\"filepath\")\n",
        "    summarize_btn = gr.Button(\"ğŸ§  Ã–zeti OluÅŸtur\")\n",
        "\n",
        "    summary_output = gr.Textbox(label=\"ğŸ“„ Ã–zet\", lines=10)\n",
        "    chat_input = gr.Textbox(label=\"â“ Sorunuzu YazÄ±n\")\n",
        "    chat_btn = gr.Button(\"ğŸ’¬ Sor\")\n",
        "    chat_output = gr.Textbox(label=\"ğŸ¤– Cevap\", lines=8)\n",
        "\n",
        "    state_vectorstore = gr.State()\n",
        "\n",
        "    def process_and_summarize(pdf_file, lang):\n",
        "        summary, vectorstore = summarize_pdf(pdf_file, lang)\n",
        "        return summary, vectorstore\n",
        "\n",
        "    summarize_btn.click(\n",
        "        process_and_summarize,\n",
        "        inputs=[pdf_input, lang_choice],\n",
        "        outputs=[summary_output, state_vectorstore]\n",
        "    )\n",
        "\n",
        "    chat_btn.click(\n",
        "        chat_with_paper,\n",
        "        inputs=[chat_input, state_vectorstore, lang_choice],\n",
        "        outputs=chat_output\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BoomeRLJ04Rr",
        "outputId": "ba6d5c76-12ec-4af5-983b-27fb99b9995e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”‘ Gemini API Key'inizi girin: AIzaSyAplGeHrM5F8DPButbbEL6iQeyQ5ZrXXgQ\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://fe8a738ea995a006e9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fe8a738ea995a006e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1133, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 123, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 109, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 389, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 288, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x78c678e57890 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://fe8a738ea995a006e9.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# DocuLingo (arayÃ¼zsÃ¼z)\n",
        "# ============================================================\n",
        "\n",
        "!pip install google-generativeai PyPDF2 langchain langchain-community langchain-google-genai chromadb python-dotenv -q\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "import google.generativeai as genai\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from google.colab import files, output\n",
        "from IPython.display import display, HTML\n",
        "import time, os\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ğŸ”‘ Gemini API Key\n",
        "# ============================================================\n",
        "GOOGLE_API_KEY = input(\"ğŸ”‘ Gemini API Key'inizi girin: \").strip()\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# ============================================================\n",
        "# ğŸ“‚ PDF YÃ¼kleme\n",
        "# ============================================================\n",
        "print(\"\\nğŸ“ LÃ¼tfen Ã¶zetlenecek PDF dosyasÄ±nÄ± yÃ¼kleyin:\")\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "\n",
        "# PDF metnini oku\n",
        "reader = PdfReader(pdf_path)\n",
        "text = \"\"\n",
        "for page in reader.pages:\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        text += content + \"\\n\"\n",
        "\n",
        "# ============================================================\n",
        "# ğŸŒ Dil SeÃ§imi (KullanÄ±cÄ±dan Buton ile)\n",
        "# ============================================================\n",
        "print(\"ğŸ“˜ Ã–zet hangi dilde olsun?\")\n",
        "print(\"1ï¸âƒ£ TÃ¼rkÃ§e\\n2ï¸âƒ£ Ä°ngilizce\")\n",
        "\n",
        "choice = input(\"SeÃ§iminizi yapÄ±n (1 veya 2): \").strip()\n",
        "if choice == \"1\":\n",
        "    chosen_lang = \"tr\"\n",
        "elif choice == \"2\":\n",
        "    chosen_lang = \"en\"\n",
        "else:\n",
        "    print(\"âŒ GeÃ§ersiz seÃ§im, varsayÄ±lan TÃ¼rkÃ§e seÃ§ildi.\")\n",
        "    chosen_lang = \"tr\"\n",
        "\n",
        "print(f\"ğŸ”¤ SeÃ§ilen dil: {chosen_lang}\")\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# âœ¨ Ã–zet OluÅŸturma (Gemini)\n",
        "# ============================================================\n",
        "model_name = \"gemini-2.0-flash\"\n",
        "model = genai.GenerativeModel(model_name, generation_config={\"temperature\": 0.3, \"max_output_tokens\": 1500})\n",
        "\n",
        "if chosen_lang == \"tr\":\n",
        "   prompt = f\"\"\"\n",
        "    AÅŸaÄŸÄ±daki akademik makalenin ayrÄ±ntÄ±lÄ± bir Ã¶zetini oluÅŸtur.\n",
        "    Ã‡alÄ±ÅŸmanÄ±n amacÄ±, kullanÄ±lan yÃ¶ntemler, veriler, bulgular ve sonuÃ§larÄ± aÃ§Ä±klayan\n",
        "    en az 6-8 cÃ¼mlelik akademik bir paragraf yaz.\n",
        "    AyrÄ±ca kullanÄ±lan modelleri veya algoritmalarÄ± da belirt.\n",
        "    TÃ¼rkÃ§e yaz.\n",
        "\n",
        "    Makale metni:\n",
        "    {text[:7000]}\n",
        "    \"\"\"\n",
        "else:\n",
        "   prompt = f\"\"\"\n",
        "    Provide a detailed summary of the following academic paper.\n",
        "    Include the research goal, methodology, data used, findings, and conclusions.\n",
        "    Write an academic-style paragraph with at least 6â€“8 sentences.\n",
        "    Also mention any models or algorithms used.\n",
        "    Write in English.\n",
        "\n",
        "    Paper text:\n",
        "    {text[:7000]}\n",
        "    \"\"\"\n",
        "\n",
        "print(\"\\nğŸ¤– Ã–zeti oluÅŸturuyor, lÃ¼tfen bekleyin...\")\n",
        "summary = model.generate_content(prompt)\n",
        "try:\n",
        "    summary = summary.text\n",
        "except:\n",
        "    summary = str(summary)\n",
        "\n",
        "print(\"\\nğŸ“„ Ã–ZET:\")\n",
        "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "print(summary)\n",
        "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "\n",
        "# ============================================================\n",
        "# ğŸ§© ChromaDB + Soru Cevap\n",
        "# ============================================================\n",
        "print(\"\\nğŸ” Makale iÃ§eriÄŸi dizine ekleniyor...\")\n",
        "CHROMA_DIR = \"chroma_db\"\n",
        "os.makedirs(CHROMA_DIR, exist_ok=True)\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "embedding_model = \"models/embedding-001\"\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# HuggingFace Ã¼cretsiz embedding modeli\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = Chroma.from_texts(chunks, embeddings, persist_directory=CHROMA_DIR)\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=model_name,\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.2,\n",
        "    system_prompt=\"You are an AI academic assistant. Only answer based on the provided paper content.\"\n",
        ")\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
        "\n",
        "print(\"\\nğŸ’¬ ArtÄ±k makaleyle ilgili sorular sorabilirsiniz!\")\n",
        "print(\"Ã‡Ä±kmak iÃ§in 'q' yazÄ±n.\\n\")\n",
        "\n",
        "while True:\n",
        "    question = input(\"â“ Soru: \")\n",
        "    if question.lower() == \"q\":\n",
        "        print(\"ğŸ”š Sohbet sonlandÄ±rÄ±ldÄ±.\")\n",
        "        break\n",
        "    result = qa.invoke({\"question\": question, \"chat_history\": []})\n",
        "    print(\"\\nğŸ§  Cevap:\\n\", result[\"answer\"])\n",
        "    print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "IJ639tFEhy1V",
        "outputId": "7f1ef939-54c4-4407-ce0d-ecd4f9ded29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”‘ Gemini API Key'inizi girin: AIzaSyAplGeHrM5F8DPButbbEL6iQeyQ5ZrXXgQ\n",
            "\n",
            "ğŸ“ LÃ¼tfen Ã¶zetlenecek PDF dosyasÄ±nÄ± yÃ¼kleyin:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a4e6be20-8425-4d67-9d10-0bc0900804ed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a4e6be20-8425-4d67-9d10-0bc0900804ed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Chapter 5 Elasticity and Its Application.pdf to Chapter 5 Elasticity and Its Application.pdf\n",
            "ğŸ“˜ Ã–zet hangi dilde olsun?\n",
            "1ï¸âƒ£ TÃ¼rkÃ§e\n",
            "2ï¸âƒ£ Ä°ngilizce\n",
            "SeÃ§iminizi yapÄ±n (1 veya 2): 2\n",
            "ğŸ”¤ SeÃ§ilen dil: en\n",
            "\n",
            "ğŸ¤– Ã–zeti oluÅŸturuyor, lÃ¼tfen bekleyin...\n",
            "\n",
            "ğŸ“„ Ã–ZET:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "This academic paper, presented as a PowerPoint by Andreea Chiritescu, explores the concept of elasticity, specifically focusing on the price elasticity of demand. The primary research goal is to define and explain the concept of price elasticity of demand, its determinants, and its implications for total revenue. The methodology employed is primarily descriptive, outlining the key concepts and providing examples to illustrate different types of demand elasticity. The data used is hypothetical, presented in the form of demand curves and numerical examples to demonstrate the calculation of price elasticity using the midpoint method. The findings highlight that the price elasticity of demand is influenced by factors such as the availability of substitutes, whether a good is a necessity or a luxury, the definition of the market, and the time horizon. The paper concludes that understanding price elasticity is crucial for businesses as it directly impacts total revenue; for instance, an inelastic demand implies that a price increase will raise total revenue, while an elastic demand suggests the opposite. The paper uses the midpoint method formula to calculate the price elasticity of demand between two points on the demand curve.\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ” Makale iÃ§eriÄŸi dizine ekleniyor...\n",
            "\n",
            "ğŸ’¬ ArtÄ±k makaleyle ilgili sorular sorabilirsiniz!\n",
            "Ã‡Ä±kmak iÃ§in 'q' yazÄ±n.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1084204386.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"â“ Soru: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"q\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ”š Sohbet sonlandÄ±rÄ±ldÄ±.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}